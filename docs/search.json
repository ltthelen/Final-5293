[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "5293 FINAL PROJECT",
    "section": "",
    "text": "1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "step1.html",
    "href": "step1.html",
    "title": "2  Step1: Repeatability",
    "section": "",
    "text": "2.1 Repeatability - Random Forest\nCode\nlibrary(randomForest)\n\n\nrandomForest 4.7-1.2\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\nCode\nlibrary(DALEX)\n\n\nWelcome to DALEX (version: 2.4.3).\nFind examples and detailed introduction at: http://ema.drwhy.ai/\n\n\nCode\nlibrary(iml)\nlibrary(mlbench)\nlibrary(caret)\n\n\nLoading required package: ggplot2\n\n\n\nAttaching package: 'ggplot2'\n\n\nThe following object is masked from 'package:randomForest':\n\n    margin\n\n\nLoading required package: lattice\n\n\nCode\ndata(PimaIndiansDiabetes)\ndf &lt;- na.omit(PimaIndiansDiabetes)\nset.seed(5293)\n\ntrain_idx &lt;- createDataPartition(df$diabetes, p = 0.8, list = FALSE)\ntrain_data &lt;- df[train_idx, ]\ntest_data &lt;- df[-train_idx, ]\n\nlime_results &lt;- list()\nshap_results &lt;- list()\n\nfor (i in 1:10) {\n  set.seed(i)\n  rf_model &lt;- randomForest(diabetes ~ ., data = train_data, ntree = 100)\n\n  # LIME\n  explainer_lime &lt;- DALEX::explain(\n    rf_model,\n    data = train_data[, -ncol(train_data)],\n    y = train_data$diabetes\n  )\n  lime_expl &lt;- predict_parts(\n    explainer_lime,\n    new_observation = test_data[1, -ncol(test_data)],\n    type = \"break_down\"\n  )\n  lime_results[[i]] &lt;- lime_expl\n\n  # SHAP\n  X &lt;- train_data[, -ncol(train_data)]\n  predictor &lt;- iml::Predictor$new(\n    rf_model,\n    data = X,\n    y = train_data$diabetes,\n    type = \"prob\"\n  )\n  shap &lt;- iml::Shapley$new(predictor, x.interest = test_data[1, -ncol(test_data)])\n  shap_results[[i]] &lt;- shap$results\n}\n\n\nPreparation of a new explainer is initiated\n  -&gt; model label       :  randomForest  (  default  )\n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  yhat.randomForest  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package randomForest , ver. 4.7.1.2 , task classification (  default  ) \n  -&gt; model_info        :  Model info detected classification task but 'y' is a factor .  (  WARNING  )\n  -&gt; model_info        :  By deafult classification tasks supports only numercical 'y' parameter. \n  -&gt; model_info        :  Consider changing to numerical vector with 0 and 1 values.\n  -&gt; model_info        :  Otherwise I will not be able to calculate residuals or loss function.\n  -&gt; predicted values  :  numerical, min =  0 , mean =  0.3471545 , max =  0.99  \n  -&gt; residual function :  difference between y and yhat (  default  )\n\n\nWarning in Ops.factor(y, predict_function(model, data)): '-' not meaningful for\nfactors\n\n\n  -&gt; residuals         :  numerical, min =  NA , mean =  NA , max =  NA  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  randomForest  (  default  )\n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  yhat.randomForest  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package randomForest , ver. 4.7.1.2 , task classification (  default  ) \n  -&gt; model_info        :  Model info detected classification task but 'y' is a factor .  (  WARNING  )\n  -&gt; model_info        :  By deafult classification tasks supports only numercical 'y' parameter. \n  -&gt; model_info        :  Consider changing to numerical vector with 0 and 1 values.\n  -&gt; model_info        :  Otherwise I will not be able to calculate residuals or loss function.\n  -&gt; predicted values  :  numerical, min =  0 , mean =  0.3489431 , max =  0.98  \n  -&gt; residual function :  difference between y and yhat (  default  )\n\n\nWarning in Ops.factor(y, predict_function(model, data)): '-' not meaningful for\nfactors\n\n\n  -&gt; residuals         :  numerical, min =  NA , mean =  NA , max =  NA  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  randomForest  (  default  )\n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  yhat.randomForest  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package randomForest , ver. 4.7.1.2 , task classification (  default  ) \n  -&gt; model_info        :  Model info detected classification task but 'y' is a factor .  (  WARNING  )\n  -&gt; model_info        :  By deafult classification tasks supports only numercical 'y' parameter. \n  -&gt; model_info        :  Consider changing to numerical vector with 0 and 1 values.\n  -&gt; model_info        :  Otherwise I will not be able to calculate residuals or loss function.\n  -&gt; predicted values  :  numerical, min =  0 , mean =  0.3501789 , max =  0.99  \n  -&gt; residual function :  difference between y and yhat (  default  )\n\n\nWarning in Ops.factor(y, predict_function(model, data)): '-' not meaningful for\nfactors\n\n\n  -&gt; residuals         :  numerical, min =  NA , mean =  NA , max =  NA  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  randomForest  (  default  )\n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  yhat.randomForest  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package randomForest , ver. 4.7.1.2 , task classification (  default  ) \n  -&gt; model_info        :  Model info detected classification task but 'y' is a factor .  (  WARNING  )\n  -&gt; model_info        :  By deafult classification tasks supports only numercical 'y' parameter. \n  -&gt; model_info        :  Consider changing to numerical vector with 0 and 1 values.\n  -&gt; model_info        :  Otherwise I will not be able to calculate residuals or loss function.\n  -&gt; predicted values  :  numerical, min =  0 , mean =  0.3503252 , max =  0.98  \n  -&gt; residual function :  difference between y and yhat (  default  )\n\n\nWarning in Ops.factor(y, predict_function(model, data)): '-' not meaningful for\nfactors\n\n\n  -&gt; residuals         :  numerical, min =  NA , mean =  NA , max =  NA  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  randomForest  (  default  )\n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  yhat.randomForest  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package randomForest , ver. 4.7.1.2 , task classification (  default  ) \n  -&gt; model_info        :  Model info detected classification task but 'y' is a factor .  (  WARNING  )\n  -&gt; model_info        :  By deafult classification tasks supports only numercical 'y' parameter. \n  -&gt; model_info        :  Consider changing to numerical vector with 0 and 1 values.\n  -&gt; model_info        :  Otherwise I will not be able to calculate residuals or loss function.\n  -&gt; predicted values  :  numerical, min =  0 , mean =  0.3474309 , max =  0.99  \n  -&gt; residual function :  difference between y and yhat (  default  )\n\n\nWarning in Ops.factor(y, predict_function(model, data)): '-' not meaningful for\nfactors\n\n\n  -&gt; residuals         :  numerical, min =  NA , mean =  NA , max =  NA  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  randomForest  (  default  )\n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  yhat.randomForest  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package randomForest , ver. 4.7.1.2 , task classification (  default  ) \n  -&gt; model_info        :  Model info detected classification task but 'y' is a factor .  (  WARNING  )\n  -&gt; model_info        :  By deafult classification tasks supports only numercical 'y' parameter. \n  -&gt; model_info        :  Consider changing to numerical vector with 0 and 1 values.\n  -&gt; model_info        :  Otherwise I will not be able to calculate residuals or loss function.\n  -&gt; predicted values  :  numerical, min =  0 , mean =  0.3496748 , max =  0.99  \n  -&gt; residual function :  difference between y and yhat (  default  )\n\n\nWarning in Ops.factor(y, predict_function(model, data)): '-' not meaningful for\nfactors\n\n\n  -&gt; residuals         :  numerical, min =  NA , mean =  NA , max =  NA  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  randomForest  (  default  )\n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  yhat.randomForest  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package randomForest , ver. 4.7.1.2 , task classification (  default  ) \n  -&gt; model_info        :  Model info detected classification task but 'y' is a factor .  (  WARNING  )\n  -&gt; model_info        :  By deafult classification tasks supports only numercical 'y' parameter. \n  -&gt; model_info        :  Consider changing to numerical vector with 0 and 1 values.\n  -&gt; model_info        :  Otherwise I will not be able to calculate residuals or loss function.\n  -&gt; predicted values  :  numerical, min =  0 , mean =  0.3470244 , max =  0.98  \n  -&gt; residual function :  difference between y and yhat (  default  )\n\n\nWarning in Ops.factor(y, predict_function(model, data)): '-' not meaningful for\nfactors\n\n\n  -&gt; residuals         :  numerical, min =  NA , mean =  NA , max =  NA  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  randomForest  (  default  )\n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  yhat.randomForest  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package randomForest , ver. 4.7.1.2 , task classification (  default  ) \n  -&gt; model_info        :  Model info detected classification task but 'y' is a factor .  (  WARNING  )\n  -&gt; model_info        :  By deafult classification tasks supports only numercical 'y' parameter. \n  -&gt; model_info        :  Consider changing to numerical vector with 0 and 1 values.\n  -&gt; model_info        :  Otherwise I will not be able to calculate residuals or loss function.\n  -&gt; predicted values  :  numerical, min =  0 , mean =  0.3438699 , max =  0.98  \n  -&gt; residual function :  difference between y and yhat (  default  )\n\n\nWarning in Ops.factor(y, predict_function(model, data)): '-' not meaningful for\nfactors\n\n\n  -&gt; residuals         :  numerical, min =  NA , mean =  NA , max =  NA  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  randomForest  (  default  )\n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  yhat.randomForest  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package randomForest , ver. 4.7.1.2 , task classification (  default  ) \n  -&gt; model_info        :  Model info detected classification task but 'y' is a factor .  (  WARNING  )\n  -&gt; model_info        :  By deafult classification tasks supports only numercical 'y' parameter. \n  -&gt; model_info        :  Consider changing to numerical vector with 0 and 1 values.\n  -&gt; model_info        :  Otherwise I will not be able to calculate residuals or loss function.\n  -&gt; predicted values  :  numerical, min =  0 , mean =  0.3456748 , max =  0.99  \n  -&gt; residual function :  difference between y and yhat (  default  )\n\n\nWarning in Ops.factor(y, predict_function(model, data)): '-' not meaningful for\nfactors\n\n\n  -&gt; residuals         :  numerical, min =  NA , mean =  NA , max =  NA  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  randomForest  (  default  )\n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  yhat.randomForest  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package randomForest , ver. 4.7.1.2 , task classification (  default  ) \n  -&gt; model_info        :  Model info detected classification task but 'y' is a factor .  (  WARNING  )\n  -&gt; model_info        :  By deafult classification tasks supports only numercical 'y' parameter. \n  -&gt; model_info        :  Consider changing to numerical vector with 0 and 1 values.\n  -&gt; model_info        :  Otherwise I will not be able to calculate residuals or loss function.\n  -&gt; predicted values  :  numerical, min =  0 , mean =  0.3498374 , max =  0.99  \n  -&gt; residual function :  difference between y and yhat (  default  )\n\n\nWarning in Ops.factor(y, predict_function(model, data)): '-' not meaningful for\nfactors\n\n\n  -&gt; residuals         :  numerical, min =  NA , mean =  NA , max =  NA  \n  A new explainer has been created!\nCode\n# LIME\n\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:DALEX':\n\n    explain\n\n\nThe following object is masked from 'package:randomForest':\n\n    combine\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(ggplot2)\n\nlime_df &lt;- lapply(1:10, function(i) {\n  df_i &lt;- lime_results[[i]]\n  data.frame(\n    seed = i,\n    variable = df_i$variable_name,\n    contribution = df_i$contribution\n  )\n}) |&gt; bind_rows()\n\n\nlime_df_clean &lt;- lime_df |&gt;\n  filter(variable != \"intercept\" & variable != \"\") |&gt;\n  mutate(variable = factor(variable))  \n\nlime_summary_clean &lt;- lime_df_clean |&gt;\n  group_by(variable) |&gt;\n  summarise(\n    mean_contribution = mean(contribution),\n    sd_contribution = sd(contribution),\n    .groups = \"drop\"\n  )\n\nprint(lime_summary_clean)\n\n\n# A tibble: 8 × 3\n  variable mean_contribution sd_contribution\n  &lt;fct&gt;                &lt;dbl&gt;           &lt;dbl&gt;\n1 age                0.0457          0.0117 \n2 glucose           -0.156           0.00608\n3 insulin           -0.0409          0.0115 \n4 mass              -0.0291          0.00747\n5 pedigree           0.00408         0.0148 \n6 pregnant          -0.0203          0.00815\n7 pressure          -0.00941         0.0105 \n8 triceps            0.0251          0.0166\nCode\nlibrary(ggplot2)\n\nggplot(lime_df_clean, aes(x = variable, y = contribution)) +\n  geom_boxplot(fill = \"red\") +\n  labs(title = \"Distribution of LIME Contributions\",\n       x = \"Variable\", y = \"Contribution\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nCode\nggplot(lime_summary_clean, aes(x = reorder(variable, -sd_contribution), y = sd_contribution)) +\n  geom_col(fill = \"blue\") +\n  labs(title = \"Standard Deviation of LIME Explanations\",\n       x = \"Variable\", y = \"Standard Deviation\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\nCode\n# SHAP\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\nshap_df &lt;- do.call(rbind, lapply(1:10, function(i) {\n  df_i &lt;- shap_results[[i]]\n  df_i$seed &lt;- i\n  df_i\n}))\n\nshap_df &lt;- shap_df |&gt;\n  rename(variable = feature,\n         contribution = phi) |&gt;\n  dplyr::select(variable, contribution, seed)\n\nshap_df &lt;- shap_df |&gt;\n  filter(variable != \"intercept\" & variable != \"\") |&gt;\n  mutate(variable = factor(variable))  \n\n\nshap_summary &lt;- shap_df |&gt;\n  group_by(variable) |&gt;\n  summarise(\n    mean_contribution = mean(contribution),\n    sd_contribution = sd(contribution),\n    .groups = \"drop\"\n  )\n\nprint(shap_summary)\n\n\n# A tibble: 8 × 3\n  variable mean_contribution sd_contribution\n  &lt;fct&gt;                &lt;dbl&gt;           &lt;dbl&gt;\n1 age               2.78e-18         0.0585 \n2 glucose           4.17e-18         0.169  \n3 insulin           7.77e-19         0.0168 \n4 mass             -2.25e-18         0.0597 \n5 pedigree          1.76e-18         0.0166 \n6 pregnant          0                0.0306 \n7 pressure         -2.60e-19         0.00953\n8 triceps           1.71e-18         0.0231\nCode\nggplot(shap_df, aes(x = variable, y = contribution)) +\n  geom_boxplot(fill = \"green\") +\n  labs(title = \"Distribution of SHAP Contributions\",\n       x = \"Variable\", y = \"Contribution\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nCode\nggplot(shap_summary, aes(x = reorder(variable, -sd_contribution), y = sd_contribution)) +\n  geom_col(fill = \"yellow\") +\n  labs(title = \"Standard Deviation of SHAP Explanations\",\n       x = \"Variable\", y = \"Standard Deviation\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Step1: Repeatability</span>"
    ]
  },
  {
    "objectID": "step1.html#repeatability-analysis---random-forest",
    "href": "step1.html#repeatability-analysis---random-forest",
    "title": "2  Step1: Repeatability",
    "section": "2.2 Repeatability Analysis - Random Forest",
    "text": "2.2 Repeatability Analysis - Random Forest\nIn this part, we trained a Random Forest classifier 10 times on the PimaIndiansDiabetes dataset. For each model, we used LIME and SHAP to explain the prediction of the same run Then we aggregated the feature-level contributions and calculated the mean and standard deviation across all runs to assess the repeatability of explanations.\nThe results show that LIME explanations vary significantly across runs, especially for features with moderate importance such as triceps and pedigree. In contrast, features like glucose exhibited relatively low standard deviation and consistent direction of contribution. For SHAP, although the mean contribution for most features was close to zero, the standard deviation was large for key features like glucose and mass. This suggests that SHAP may be more balanced in average attribution but more sensitive to model perturbations.\nIn summary, both LIME and SHAP explanations were affected by the choice of random seed, demonstrating non-negligible variability in feature attribution.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Step1: Repeatability</span>"
    ]
  },
  {
    "objectID": "step1.html#repeatability---logistic-regression",
    "href": "step1.html#repeatability---logistic-regression",
    "title": "2  Step1: Repeatability",
    "section": "2.3 Repeatability - Logistic Regression",
    "text": "2.3 Repeatability - Logistic Regression\n\n\nCode\n#install.packages(c(\"mlbench\", \"caret\", \"DALEX\", \"DALEXtra\", \"iml\", \"lime\",\"dplyr\", \"ggplot2\", \"ggpubr\"), dependencies = TRUE)\n\nlibrary(mlbench)\nlibrary(caret)\nlibrary(DALEX)\nlibrary(DALEXtra)\nlibrary(iml)\nlibrary(lime)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\n\n\nCode\nmodel_type.glm &lt;- function(x, ...) \"classification\"\npredict_model.glm &lt;- function(x, newdata, ...) {\n  preds &lt;- predict(x, newdata, type = \"response\")\n  data.frame(`No` = 1 - preds, `Yes` = preds)\n}\n\n\ndata(PimaIndiansDiabetes)\ndf &lt;- na.omit(PimaIndiansDiabetes)\nset.seed(5293)\ndf$diabetes &lt;- factor(df$diabetes)\nX &lt;- df[, -ncol(df)]\ny &lt;- df$diabetes\n\nlime_contributions &lt;- list()\nshap_contributions &lt;- list()\n\n\ntrain_idx &lt;- createDataPartition(df$diabetes, p = 0.8, list = FALSE)\ntrain_data &lt;- df[train_idx, ]\ntest_data &lt;- df[-train_idx, ]\n\n\nIn the first step, we evaluate the repeatability of LIME explanations by repeatedly training the model and generating explanations to understand their consistency.\nWe conduct self-sampling (bootstrap) on the training data to obtain multiple different training sets. Each time, we train the logistic regression model on the newly sampled data and run the LIME interpretation model prediction with the same test samples, and calculate the standard deviation of the contribution value of each feature in multiple runs.\n\n\nCode\nn_runs &lt;- 10\nlime_contribs &lt;- list()\n\nfor (i in 1:n_runs) {\n  boot_idx &lt;- sample(nrow(train_data), replace = TRUE)\n  boot_train &lt;- train_data[boot_idx, ]\n\n  logit_model &lt;- glm(diabetes ~ ., data = boot_train, family = binomial)\n\n  explainer &lt;- DALEX::explain(\n    model = logit_model,\n    data = boot_train[, -ncol(boot_train)],\n    y = as.numeric(boot_train$diabetes == \"pos\"),\n    label = paste0(\"glm_\", i),\n    predict_function = function(m, d) predict(m, d, type = \"response\")\n  )\n\n  lime_result &lt;- predict_parts(explainer, new_observation = test_data[1, -ncol(test_data)],\n                               type = \"break_down\")\n  lime_df &lt;- lime_result %&gt;%\n    filter(variable != \"intercept\") %&gt;%\n    select(variable, contribution)\n\n  lime_contribs[[i]] &lt;- lime_df\n}\n\n\nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_1 \n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  function(m, d) predict(m, d, type = \"response\") \n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.001490493 , mean =  0.3593496 , max =  0.9958617  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.9534075 , mean =  -3.827759e-13 , max =  0.9856082  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_2 \n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  function(m, d) predict(m, d, type = \"response\") \n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.002668286 , mean =  0.3219512 , max =  0.978063  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.8972503 , mean =  -2.019077e-13 , max =  0.9888889  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_3 \n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  function(m, d) predict(m, d, type = \"response\") \n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.002188936 , mean =  0.3447154 , max =  0.9860676  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.8505019 , mean =  -5.432463e-15 , max =  0.9907279  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_4 \n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  function(m, d) predict(m, d, type = \"response\") \n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.0004471588 , mean =  0.3317073 , max =  0.9980853  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.94405 , mean =  -1.474228e-10 , max =  0.9947934  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_5 \n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  function(m, d) predict(m, d, type = \"response\") \n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.001453277 , mean =  0.3495935 , max =  0.9830114  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.9333344 , mean =  -2.370711e-15 , max =  0.9933661  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_6 \n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  function(m, d) predict(m, d, type = \"response\") \n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.004191668 , mean =  0.3447154 , max =  0.9763506  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.9545668 , mean =  -1.842577e-13 , max =  0.9958083  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_7 \n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  function(m, d) predict(m, d, type = \"response\") \n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.004528123 , mean =  0.3252033 , max =  0.9956203  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.9475119 , mean =  -9.104453e-15 , max =  0.983609  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_8 \n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  function(m, d) predict(m, d, type = \"response\") \n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.003427715 , mean =  0.3560976 , max =  0.9924571  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.9353339 , mean =  -3.663064e-16 , max =  0.9104446  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_9 \n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  function(m, d) predict(m, d, type = \"response\") \n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.007305473 , mean =  0.3658537 , max =  0.9946787  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.9522851 , mean =  -6.758618e-14 , max =  0.9577366  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_10 \n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  function(m, d) predict(m, d, type = \"response\") \n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.001906982 , mean =  0.3284553 , max =  0.9547832  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.9030004 , mean =  -1.521891e-15 , max =  0.9934916  \n  A new explainer has been created!  \n\n\nCode\nlime_summary &lt;- bind_rows(lime_contribs, .id = \"run\") %&gt;%\n  group_by(variable) %&gt;%\n  summarise(sd_contribution = sd(contribution), .groups = \"drop\")\n\nlibrary(ggplot2)\nggplot(lime_summary, aes(x = reorder(variable, -sd_contribution), y = sd_contribution)) +\n  geom_col(fill = \"steelblue\") +\n  labs(title = \"LIME Repeatability (SD of Contributions)\", x = \"Variable\", y = \"SD\") +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Step1: Repeatability</span>"
    ]
  },
  {
    "objectID": "step1.html#repeatability-analysis---logistic-regression",
    "href": "step1.html#repeatability-analysis---logistic-regression",
    "title": "2  Step1: Repeatability",
    "section": "2.4 Repeatability Analysis - Logistic Regression",
    "text": "2.4 Repeatability Analysis - Logistic Regression\nThe column height indicates the standard deviation of the LIME contribution for each feature across different model runs. A higher column means greater variation in the feature’s attribution, indicating worse repeatability, a lower column implies more consistent contributions, indicating greater stability.\nWe found that LIME explanations were more sensitive to training data changes for features like mass and glucose, which had the highest standard deviations in contribution across runs. However, features such as pressure, age, and pedigree showed very low variability, with consistently stable contributions across repeated model trainings. This suggests that for these features, LIME explanations are highly repeatable and robust.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Step1: Repeatability</span>"
    ]
  }
]